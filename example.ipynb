{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA v2.0 Interactive Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8-sns_THhYF"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import importlib\n",
    "import os\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGgXowHlLKmz"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU with CUDA support is required\")\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OE9dAzgiHs97",
    "outputId": "044f0a89-42ca-478c-8b30-2e563119b840"
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "if not os.path.exists(\"./utils\"):\n",
    "    !git clone https://github.com/ZhangShaozuo/Artificial_Intelligence_VQA.git\n",
    "    !ln -s ./DL-BigProject-VQA/utils ./utils\n",
    "\n",
    "if os.path.exists(\"./DL-BigProject-VQA/utils\"):\n",
    "    !cd ./DL-BigProject-VQA/utils && git pull\n",
    "\n",
    "import utils.data as data_util\n",
    "import utils.helper as helper\n",
    "import utils.train as train_util\n",
    "from utils.vocab import Vocab\n",
    "\n",
    "importlib.reload(data_util)\n",
    "importlib.reload(helper)\n",
    "importlib.reload(train_util)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwV9bL9_F4LQ"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI12pREDEINt"
   },
   "outputs": [],
   "source": [
    "# Load dataset (single word answer only)\n",
    "image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(int(224 / 0.875)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def tokenizer(question: str) -> List[str]:\n",
    "    # to lower case\n",
    "    question = question.lower()\n",
    "    # remove punctuation\n",
    "    trans = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    question = question.translate(trans)\n",
    "    # split words\n",
    "    return question.split()\n",
    "\n",
    "\n",
    "question_vocab = Vocab({})\n",
    "\n",
    "\n",
    "def question_transform(question: str):\n",
    "    tokens = tokenizer(question)\n",
    "    indices = [question_vocab[token] for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "def question_transform_factory(corpus: Iterable[str]):\n",
    "    global question_vocab\n",
    "    counter = Counter()\n",
    "    for text in corpus:\n",
    "        counter.update(tokenizer(text))\n",
    "    question_vocab = Vocab(counter, specials=[\"<pad>\", \"<unk>\"])\n",
    "    return question_transform\n",
    "\n",
    "\n",
    "answer_vocab = Vocab({})\n",
    "\n",
    "\n",
    "def answer_tansform(answer: str):\n",
    "    return answer_vocab[answer]\n",
    "\n",
    "\n",
    "def answer_tansform_factory(corpus: Iterable[str]):\n",
    "    global answer_vocab\n",
    "    answer_vocab = Vocab(Counter(corpus), specials=[\"<unk>\"], min_freq=10)\n",
    "    return answer_tansform\n",
    "\n",
    "\n",
    "train_dataset = data_util.VQA2Dataset(\n",
    "    \"./VQA2/\",\n",
    "    group=\"train\",\n",
    "    image_transform=image_transform,\n",
    "    question_transform_factory=question_transform_factory,\n",
    "    answer_transform_factory=answer_tansform_factory,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "valid_dataset = data_util.VQA2Dataset(\n",
    "    \"./VQA2/\",\n",
    "    group=\"val\",\n",
    "    image_transform=image_transform,\n",
    "    question_transform=question_transform,\n",
    "    answer_transform=answer_tansform,\n",
    ")\n",
    "\n",
    "test_dataset = data_util.VQA2Dataset(\n",
    "    \"./VQA2/\",\n",
    "    group=\"test\",\n",
    "    image_transform=image_transform,\n",
    "    question_transform=question_transform,\n",
    "    answer_transform=answer_tansform,\n",
    ")\n",
    "\n",
    "print(\"train_dataset:\", len(train_dataset))\n",
    "print(\"valid_dataset:\", len(valid_dataset))\n",
    "print(\"test_dataset:\", len(test_dataset))\n",
    "print()\n",
    "print(\"quesiton_vocab size:\", len(question_vocab))\n",
    "print(\"answer_vocab size:  \", len(answer_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocab\n",
    "question_vocab.save(\"question_vocab.json\")\n",
    "answer_vocab.save(\"answer_vocab.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab\n",
    "question_vocab = Vocab.load(\"question_vocab.json\")\n",
    "answer_vocab = Vocab.load(\"answer_vocab.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0agablT8Cr4o"
   },
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N43OSwolHzK1"
   },
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "batch_size = 64\n",
    "PAD_IDX = question_vocab[\"<pad>\"]\n",
    "\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    data_batch.sort(key=lambda x: -len(x[1]))  # for pack_padded_sequence\n",
    "    images, questions, answers = zip(*data_batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    q_lengths = [len(q) for q in questions]\n",
    "    questions = rnn_utils.pad_sequence(questions, padding_value=PAD_IDX)\n",
    "    answers = torch.tensor(answers, dtype=torch.long)\n",
    "    return images, questions, q_lengths, answers\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=generate_batch, shuffle=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, collate_fn=generate_batch\n",
    ")\n",
    "\n",
    "# use a subset of the validation dataset\n",
    "mini_valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=generate_batch,\n",
    "    sampler=SubsetRandomSampler(list(range(512))),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=generate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TL605lVnID7_",
    "outputId": "423b98d4-0758-4e1a-f460-458868d17dd3"
   },
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "def sequence_to_sentence(seq: List[int]) -> str:\n",
    "    sentence = []\n",
    "    for i in seq:\n",
    "        if i == PAD_IDX:\n",
    "            break\n",
    "        sentence.append(question_vocab.itos[i])\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "\n",
    "def visualize_samples(images, questions, answers, max_num=-1):\n",
    "    if max_num < 0:\n",
    "        max_num = len(images)\n",
    "\n",
    "    # PyTorch RNN is using (seq_len, batch, input_size)\n",
    "    # make it (batch, seq_len, input_size)\n",
    "    questions = questions.transpose(0, 1)\n",
    "\n",
    "    for _, v, q, a in zip(range(max_num), images, questions, answers):\n",
    "        print(\"Q:\", sequence_to_sentence(q))\n",
    "        print(\"A:\", answer_vocab.itos[a])\n",
    "        helper.imshow(v)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "images, questions, _, answers = next(iter(valid_loader))\n",
    "visualize_samples(images, questions, answers, max_num=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNet(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.vgg16(pretrained=True)\n",
    "        model.classifier = nn.Linear(512 * 7 * 7, 256)\n",
    "        self.backbone = model\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, vocab, embedding_dim, out_dim, weights_path=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.rnn = nn.GRU(embedding_dim, out_dim)\n",
    "\n",
    "        if weights_path:\n",
    "            counter = 0\n",
    "            weights = self.embedding.weight.detach().numpy()\n",
    "            with open(weights_path, encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    elements = line.split(\" \")\n",
    "\n",
    "                    word = elements[0]\n",
    "                    if word not in question_vocab.stoi:\n",
    "                        continue\n",
    "\n",
    "                    embed = np.asarray(elements[1:], dtype=\"float32\")\n",
    "                    weights[question_vocab.stoi[word]] = embed\n",
    "\n",
    "                    counter += 1\n",
    "                    if counter / len(question_vocab) > 0.9:\n",
    "                        break\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(weights))\n",
    "\n",
    "    def forward(self, q, q_len):\n",
    "        embedded = self.embedding(q)\n",
    "        tanhed = self.tanh(embedded)\n",
    "        packed = rnn_utils.pack_padded_sequence(tanhed, q_len)\n",
    "        _, features = self.rnn(packed)\n",
    "        features = features.squeeze(0)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSUARenZIGfV"
   },
   "outputs": [],
   "source": [
    "class VQANet(nn.Module):\n",
    "    def __init__(self, vocab, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.v_net = VNet()\n",
    "        self.q_net = QNet(vocab, 100, 256)\n",
    "\n",
    "        self.v_query = nn.Sequential(nn.Conv2d(256, 128, 1), nn.Sigmoid())\n",
    "        self.q_query = nn.Sequential(nn.Linear(256, 128), nn.Sigmoid())\n",
    "        self.attention_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.q_fc = nn.Sequential(nn.Linear(256, 256), nn.Sigmoid())\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512), nn.ReLU(True), nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.last_attention = None\n",
    "\n",
    "    def forward(self, v, q, q_len):\n",
    "        v_feat = self.v_net(v)\n",
    "        q_feat = self.q_net(q, q_len)\n",
    "\n",
    "#         v_query = self.v_query(v_feat)\n",
    "#         q_query = self.q_query(q_feat)\n",
    "#         attention = (v_query * q_query.view((-1, 128, 1, 1))).sum(dim=1)\n",
    "#         attention = self.attention_softmax(attention.view(-1, 7 * 7)).view(-1, 1, 7, 7)\n",
    "#         self.last_attention = attention.detach()  # save for visualization\n",
    "\n",
    "#         v_final = (v_feat * attention).view((-1, 256, 7 * 7)).sum(dim=2)\n",
    "#         q_final = self.q_fc(q_feat)\n",
    "        out = self.classifier(v_feat * q_feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQANet(question_vocab, len(answer_vocab)).to(device)\n",
    "for p in model.v_net.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgpwisrcIH2s",
    "outputId": "bddde40a-5390-493f-9f42-7187b40718c9"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "history = train_util.train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    mini_valid_loader,\n",
    "    epochs=8,\n",
    "    valid_every=200,\n",
    ")\n",
    "train_util.plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_util.plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the mini validation set\n",
    "mini_valid_loss, mini_valid_accu, mini_valid_topk_accu = train_util.validate_model(\n",
    "    model, mini_valid_loader, nn.CrossEntropyLoss()\n",
    ")\n",
    "print(\"loss:\", mini_valid_loss)\n",
    "print(\"accu:\", mini_valid_accu)\n",
    "print(\"topk_accu:\", mini_valid_topk_accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the full validation set\n",
    "valid_loss, valid_accu, valid_topk_accu = train_util.validate_model(\n",
    "    model, valid_loader, nn.CrossEntropyLoss(), show_progress=True\n",
    ")\n",
    "print(\"loss:\", valid_loss)\n",
    "print(\"accu:\", valid_accu)\n",
    "print(\"topk_accu:\", valid_topk_accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the test set\n",
    "test_loss, test_accu, test_topk_accu = train_util.validate_model(\n",
    "    model, test_loader, nn.CrossEntropyLoss(), show_progress=True\n",
    ")\n",
    "print(\"loss:\", test_loss)\n",
    "print(\"accu:\", test_accu)\n",
    "print(\"topk_accu:\", test_topk_accu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some samples\n",
    "images, questions, question_lengths, answers = next(iter(valid_loader))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(images.to(device), questions.to(device), question_lengths)\n",
    "predictions = torch.argmax(outputs, dim=1).cpu()\n",
    "\n",
    "questions = questions.transpose(0, 1)\n",
    "for _, v, q, a, pred in zip(range(4), images, questions, answers, predictions):\n",
    "    print(\"Q:\", sequence_to_sentence(q))\n",
    "    print(\"A:\", answer_vocab.itos[a])\n",
    "    print(\"Model:\", answer_vocab.itos[pred])\n",
    "    helper.imshow(v)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(loader):\n",
    "    images, questions, question_length, answers = next(iter(loader))\n",
    "    predictions = (\n",
    "        model(images.to(device), questions.to(device), question_length).detach().cpu()\n",
    "    )\n",
    "    attenions = model.last_attention.detach().cpu()\n",
    "    questions = questions.transpose(0, 1)\n",
    "\n",
    "    for _, v, q, a, pred, atten in zip(\n",
    "        range(4), images, questions, answers, predictions, attenions\n",
    "    ):\n",
    "        print(\"Q:\", sequence_to_sentence(q))\n",
    "        print(\"A:\", answer_vocab.itos[a])\n",
    "        print(\"model:\", answer_vocab.itos[pred.argmax(0)])\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        helper.imshow(v, ax1)\n",
    "        ax2.imshow(atten.squeeze(0))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "visualize_attention(valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL VQA Starter",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
