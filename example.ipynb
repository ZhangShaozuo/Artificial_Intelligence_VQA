{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# VQA v2.0 Interactive Notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import libraries\r\n",
    "import importlib\r\n",
    "import os\r\n",
    "import string\r\n",
    "from collections import Counter\r\n",
    "from typing import Iterable, List\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.utils.rnn as rnn_utils\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\r\n",
    "from torchvision import transforms\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "n8-sns_THhYF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check GPU\r\n",
    "if not torch.cuda.is_available():\r\n",
    "    raise RuntimeError(\"GPU with CUDA support is required\")\r\n",
    "device = torch.device(\"cuda\")\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "sGgXowHlLKmz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import utils\r\n",
    "if not os.path.exists(\"./utils\"):\r\n",
    "    !git clone https://github.com/ZhangShaozuo/Artificial_Intelligence_VQA.git\r\n",
    "    !ln -s ./DL-BigProject-VQA/utils ./utils\r\n",
    "\r\n",
    "if os.path.exists(\"./DL-BigProject-VQA/utils\"):\r\n",
    "    !cd ./DL-BigProject-VQA/utils && git pull\r\n",
    "\r\n",
    "import utils.data as data_util\r\n",
    "import utils.helper as helper\r\n",
    "import utils.train as train_util\r\n",
    "from utils.vocab import Vocab\r\n",
    "\r\n",
    "importlib.reload(data_util)\r\n",
    "importlib.reload(helper)\r\n",
    "importlib.reload(train_util)\r\n",
    "pass\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OE9dAzgiHs97",
    "outputId": "044f0a89-42ca-478c-8b30-2e563119b840"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "id": "VwV9bL9_F4LQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load dataset (single word answer only)\r\n",
    "image_transform = transforms.Compose(\r\n",
    "    [\r\n",
    "        transforms.Resize(int(224 / 0.875)),\r\n",
    "        transforms.CenterCrop(224),\r\n",
    "        transforms.ToTensor(),\r\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\r\n",
    "    ]\r\n",
    ")\r\n",
    "\r\n",
    "\r\n",
    "def tokenizer(question: str) -> List[str]:\r\n",
    "    # to lower case\r\n",
    "    question = question.lower()\r\n",
    "    # remove punctuation\r\n",
    "    trans = str.maketrans(\"\", \"\", string.punctuation)\r\n",
    "    question = question.translate(trans)\r\n",
    "    # split words\r\n",
    "    return question.split()\r\n",
    "\r\n",
    "\r\n",
    "question_vocab = Vocab({})\r\n",
    "\r\n",
    "\r\n",
    "def question_transform(question: str):\r\n",
    "    tokens = tokenizer(question)\r\n",
    "    indices = [question_vocab[token] for token in tokens]\r\n",
    "    return torch.tensor(indices, dtype=torch.long)\r\n",
    "\r\n",
    "\r\n",
    "def question_transform_factory(corpus: Iterable[str]):\r\n",
    "    global question_vocab\r\n",
    "    counter = Counter()\r\n",
    "    for text in corpus:\r\n",
    "        counter.update(tokenizer(text))\r\n",
    "    question_vocab = Vocab(counter, specials=[\"<pad>\", \"<unk>\"])\r\n",
    "    return question_transform\r\n",
    "\r\n",
    "\r\n",
    "answer_vocab = Vocab({})\r\n",
    "\r\n",
    "\r\n",
    "def answer_tansform(answer: str):\r\n",
    "    return answer_vocab[answer]\r\n",
    "\r\n",
    "\r\n",
    "def answer_tansform_factory(corpus: Iterable[str]):\r\n",
    "    global answer_vocab\r\n",
    "    answer_vocab = Vocab(Counter(corpus), specials=[\"<unk>\"], min_freq=10)\r\n",
    "    return answer_tansform\r\n",
    "\r\n",
    "\r\n",
    "train_dataset = data_util.VQA2Dataset(\r\n",
    "    \"./VQA2/\",\r\n",
    "    group=\"train\",\r\n",
    "    image_transform=image_transform,\r\n",
    "    question_transform_factory=question_transform_factory,\r\n",
    "    answer_transform_factory=answer_tansform_factory,\r\n",
    "    download=True,\r\n",
    ")\r\n",
    "\r\n",
    "valid_dataset = data_util.VQA2Dataset(\r\n",
    "    \"./VQA2/\",\r\n",
    "    group=\"val\",\r\n",
    "    image_transform=image_transform,\r\n",
    "    question_transform=question_transform,\r\n",
    "    answer_transform=answer_tansform,\r\n",
    ")\r\n",
    "\r\n",
    "test_dataset = data_util.VQA2Dataset(\r\n",
    "    \"./VQA2/\",\r\n",
    "    group=\"test\",\r\n",
    "    image_transform=image_transform,\r\n",
    "    question_transform=question_transform,\r\n",
    "    answer_transform=answer_tansform,\r\n",
    ")\r\n",
    "\r\n",
    "print(\"train_dataset:\", len(train_dataset))\r\n",
    "print(\"valid_dataset:\", len(valid_dataset))\r\n",
    "print(\"test_dataset:\", len(test_dataset))\r\n",
    "print()\r\n",
    "print(\"quesiton_vocab size:\", len(question_vocab))\r\n",
    "print(\"answer_vocab size:  \", len(answer_vocab))\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "FI12pREDEINt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save vocab\r\n",
    "question_vocab.save(\"question_vocab.json\")\r\n",
    "answer_vocab.save(\"answer_vocab.json\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load vocab\r\n",
    "question_vocab = Vocab.load(\"question_vocab.json\")\r\n",
    "answer_vocab = Vocab.load(\"answer_vocab.json\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dataloader"
   ],
   "metadata": {
    "id": "0agablT8Cr4o"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create dataloader\r\n",
    "batch_size = 64\r\n",
    "PAD_IDX = question_vocab[\"<pad>\"]\r\n",
    "\r\n",
    "\r\n",
    "def generate_batch(data_batch):\r\n",
    "    data_batch.sort(key=lambda x: -len(x[1]))  # for pack_padded_sequence\r\n",
    "    images, questions, answers = zip(*data_batch)\r\n",
    "    images = torch.stack(images, 0)\r\n",
    "    q_lengths = [len(q) for q in questions]\r\n",
    "    questions = rnn_utils.pad_sequence(questions, padding_value=PAD_IDX)\r\n",
    "    answers = torch.tensor(answers, dtype=torch.long)\r\n",
    "    return images, questions, q_lengths, answers\r\n",
    "\r\n",
    "\r\n",
    "train_loader = DataLoader(\r\n",
    "    train_dataset, batch_size=batch_size, collate_fn=generate_batch, shuffle=True\r\n",
    ")\r\n",
    "\r\n",
    "valid_loader = DataLoader(\r\n",
    "    valid_dataset, batch_size=batch_size, collate_fn=generate_batch, shuffle=True\r\n",
    ")\r\n",
    "\r\n",
    "# use a subset of the validation dataset\r\n",
    "mini_valid_loader = DataLoader(\r\n",
    "    valid_dataset,\r\n",
    "    batch_size=batch_size,\r\n",
    "    collate_fn=generate_batch,\r\n",
    "    sampler=SubsetRandomSampler(list(range(512))),\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "N43OSwolHzK1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize some samples\r\n",
    "def sequence_to_sentence(seq: List[int]) -> str:\r\n",
    "    sentence = []\r\n",
    "    for i in seq:\r\n",
    "        if i == PAD_IDX:\r\n",
    "            break\r\n",
    "        sentence.append(question_vocab.itos[i])\r\n",
    "    return \" \".join(sentence)\r\n",
    "\r\n",
    "\r\n",
    "def visualize_samples(images, questions, answers, max_num=-1):\r\n",
    "    if max_num < 0:\r\n",
    "        max_num = len(images)\r\n",
    "\r\n",
    "    # PyTorch RNN is using (seq_len, batch, input_size)\r\n",
    "    # make it (batch, seq_len, input_size)\r\n",
    "    questions = questions.transpose(0, 1)\r\n",
    "\r\n",
    "    for _, v, q, a in zip(range(max_num), images, questions, answers):\r\n",
    "        print(\"Q:\", sequence_to_sentence(q))\r\n",
    "        print(\"A:\", answer_vocab.itos[a])\r\n",
    "        helper.imshow(v)\r\n",
    "        plt.show()\r\n",
    "\r\n",
    "\r\n",
    "images, questions, _, answers = next(iter(train_loader))\r\n",
    "visualize_samples(images, questions, answers, max_num=4)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TL605lVnID7_",
    "outputId": "423b98d4-0758-4e1a-f460-458868d17dd3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class VNet(nn.Module):\r\n",
    "    def __init__(self, pretrained=True):\r\n",
    "        super().__init__()\r\n",
    "        self.backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(\r\n",
    "            pretrained=pretrained\r\n",
    "        ).backbone\r\n",
    "        self.out_channels = self.backbone.out_channels\r\n",
    "\r\n",
    "    def forward(self, images):\r\n",
    "        return self.backbone(images)[\"3\"]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class QNet(nn.Module):\r\n",
    "    def __init__(self, vocab, embedding_dim, out_dim, weights_path=None):\r\n",
    "        super().__init__()\r\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\r\n",
    "        self.tanh = nn.Tanh()\r\n",
    "        self.rnn = nn.LSTM(embedding_dim, out_dim)\r\n",
    "\r\n",
    "        if weights_path:\r\n",
    "            counter = 0\r\n",
    "            weights = self.embedding.weight.detach().numpy()\r\n",
    "            with open(weights_path, encoding=\"utf-8\") as f:\r\n",
    "                for line in f:\r\n",
    "                    elements = line.split(\" \")\r\n",
    "\r\n",
    "                    word = elements[0]\r\n",
    "                    if word not in question_vocab.stoi:\r\n",
    "                        continue\r\n",
    "\r\n",
    "                    embed = np.asarray(elements[1:], dtype=\"float32\")\r\n",
    "                    weights[question_vocab.stoi[word]] = embed\r\n",
    "\r\n",
    "                    counter += 1\r\n",
    "                    if counter / len(question_vocab) > 0.9:\r\n",
    "                        break\r\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(weights))\r\n",
    "\r\n",
    "    def forward(self, q, q_len):\r\n",
    "        embedded = self.embedding(q)\r\n",
    "        tanhed = self.tanh(embedded)\r\n",
    "        packed = rnn_utils.pack_padded_sequence(tanhed, q_len)\r\n",
    "        _, (_, features) = self.rnn(packed)\r\n",
    "        features = features.squeeze(0)\r\n",
    "        return features\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class VQANet(nn.Module):\r\n",
    "    def __init__(self, vocab, num_classes: int):\r\n",
    "        super().__init__()\r\n",
    "        self.v_net = VNet()\r\n",
    "        self.q_net = QNet(vocab, 100, 256)\r\n",
    "\r\n",
    "        self.v_query = nn.Sequential(nn.Conv2d(256, 128, 1), nn.Sigmoid())\r\n",
    "        self.q_query = nn.Sequential(nn.Linear(256, 128), nn.Sigmoid())\r\n",
    "        self.attention_softmax = nn.Softmax(dim=1)\r\n",
    "\r\n",
    "        self.q_fc = nn.Sequential(nn.Linear(256, 256), nn.Sigmoid())\r\n",
    "\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Linear(256, 512), nn.ReLU(True), nn.Linear(512, num_classes)\r\n",
    "        )\r\n",
    "        self.last_attention = None\r\n",
    "\r\n",
    "    def forward(self, v, q, q_len):\r\n",
    "        v_feat = self.v_net(v)\r\n",
    "        q_feat = self.q_net(q, q_len)\r\n",
    "\r\n",
    "        v_query = self.v_query(v_feat)\r\n",
    "        q_query = self.q_query(q_feat)\r\n",
    "        attention = (v_query * q_query.view((-1, 128, 1, 1))).sum(dim=1)\r\n",
    "        attention = self.attention_softmax(attention.view(-1, 7 * 7)).view(-1, 1, 7, 7)\r\n",
    "        self.last_attention = attention.detach()  # save for visualization\r\n",
    "\r\n",
    "        v_final = (v_feat * attention).view((-1, 256, 7 * 7)).sum(dim=2)\r\n",
    "        q_final = self.q_fc(q_feat)\r\n",
    "        out = self.classifier(v_final * q_final)\r\n",
    "        return out\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "lSUARenZIGfV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = VQANet(question_vocab, len(answer_vocab)).to(device)\r\n",
    "for p in model.v_net.parameters():\r\n",
    "    p.requires_grad = False\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "criterion = nn.CrossEntropyLoss()\r\n",
    "optimizer = optim.Adam(model.parameters())\r\n",
    "\r\n",
    "history = train_util.train_model(\r\n",
    "    model,\r\n",
    "    criterion,\r\n",
    "    optimizer,\r\n",
    "    train_loader,\r\n",
    "    mini_valid_loader,\r\n",
    "    epochs=10,\r\n",
    "    valid_every=200,\r\n",
    ")\r\n",
    "train_util.plot_history(history)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rgpwisrcIH2s",
    "outputId": "bddde40a-5390-493f-9f42-7187b40718c9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_util.plot_history(history)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Accuracy on the validation set\r\n",
    "valid_loss, valid_accu = train_util.validate_model(\r\n",
    "    model, mini_valid_loader, nn.CrossEntropyLoss()\r\n",
    ")\r\n",
    "print(\"loss:\", valid_loss)\r\n",
    "print(\"accu:\", valid_accu)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot some samples\r\n",
    "images, questions, question_lengths, answers = next(iter(train_loader))\r\n",
    "model.eval()\r\n",
    "with torch.no_grad():\r\n",
    "    outputs = model(images.to(device), questions.to(device), question_lengths)\r\n",
    "predictions = torch.argmax(outputs, dim=1).cpu()\r\n",
    "\r\n",
    "questions = questions.transpose(0, 1)\r\n",
    "for _, v, q, a, pred in zip(range(4), images, questions, answers, predictions):\r\n",
    "    print(\"Q:\", sequence_to_sentence(q))\r\n",
    "    print(\"A:\", answer_vocab.itos[a])\r\n",
    "    print(\"Model:\", answer_vocab.itos[pred])\r\n",
    "    helper.imshow(v)\r\n",
    "    plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention Visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def visualize_attention(loader):\r\n",
    "    images, questions, question_length, answers = next(iter(loader))\r\n",
    "    predictions = (\r\n",
    "        model(images.to(device), questions.to(device), question_length).detach().cpu()\r\n",
    "    )\r\n",
    "    attenions = model.last_attention.detach().cpu()\r\n",
    "    questions = questions.transpose(0, 1)\r\n",
    "\r\n",
    "    for _, v, q, a, pred, atten in zip(\r\n",
    "        range(4), images, questions, answers, predictions, attenions\r\n",
    "    ):\r\n",
    "        print(\"Q:\", sequence_to_sentence(q))\r\n",
    "        print(\"A:\", answer_vocab.itos[a])\r\n",
    "        print(\"model:\", answer_vocab.itos[pred.argmax(0)])\r\n",
    "        fig = plt.figure()\r\n",
    "        ax1 = fig.add_subplot(121)\r\n",
    "        ax2 = fig.add_subplot(122)\r\n",
    "        helper.imshow(v, ax1)\r\n",
    "        ax2.imshow(atten.squeeze(0))\r\n",
    "        plt.show()\r\n",
    "\r\n",
    "\r\n",
    "visualize_attention(train_loader)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL VQA Starter",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python377jvsc74a57bd027f60f43d6fd737251d3b74e38568705421838d6334943a9e1baec638f06b46a",
   "display_name": "Python 3.7.7 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}